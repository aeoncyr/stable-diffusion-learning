{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Lesson 4: Advanced Science Lab\n",
    "\n",
    "Welcome to the advanced lab! Here we will perform scientific experiments on the model internals.\n",
    "\n",
    "### Experiments:\n",
    "1. **Scheduler Shootout**: Euler vs DPM++ 2M\n",
    "2. **LoRA Inspection**: Peeking inside the fine-tuning weights\n",
    "3. **Step-by-Step Visualization**: Watching the image emerge from noise\n",
    "4. **The Math of Guidance**: Understanding CFG vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from core.pipeline import pipeline_manager\n",
    "from diffusers import EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scheduler Shootout: Euler vs DPM++\n",
    "\n",
    "Schedulers solve the differential equation to generate the image. Let's compare two popular ones:\n",
    "- **Euler Ancestral**: Adds random noise each step. \"Creative\" but less accurate.\n",
    "- **DPM++ 2M Karras**: Uses 2nd order math to approximate the curve. Fast and smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a macro photo of a mechanical eye, clockwork, steampunk, 8k, detailed\"\n",
    "steps = 20 # Low step count to emphasize efficiency difference\n",
    "seed = 42\n",
    "\n",
    "pipe = pipeline_manager.get_txt2img_pipeline()\n",
    "generator = torch.Generator(device=pipe.device).manual_seed(seed)\n",
    "\n",
    "# 1. Run with Euler Ancestral\n",
    "print(\"Running Euler Ancestral...\")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "image_euler = pipe(prompt=prompt, num_inference_steps=steps, generator=generator).images[0]\n",
    "\n",
    "# 2. Run with DPM++ 2M Karras\n",
    "print(\"Running DPM++ 2M Karras...\")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    pipe.scheduler.config, \n",
    "    use_karras_sigmas=True, \n",
    "    algorithm_type=\"sde-dpmsolver++\"\n",
    ")\n",
    "generator = torch.Generator(device=pipe.device).manual_seed(seed) # Reset seed\n",
    "image_dpm = pipe(prompt=prompt, num_inference_steps=steps, generator=generator).images[0]\n",
    "\n",
    "# Compare\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "ax[0].imshow(image_euler)\n",
    "ax[0].set_title(\"Euler Ancestral (20 steps)\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(image_dpm)\n",
    "ax[1].set_title(\"DPM++ 2M Karras (20 steps)\")\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRA Inspection\n",
    "\n",
    "LoRA (Low-Rank Adaptation) works by adding small matrices to the big model. Let's see them!\n",
    "\n",
    "$$W' = W + B \\cdot A$$\n",
    "\n",
    "We can inspect the fine-tuning code to see these matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tuning.lora_trainer import LoRATrainer\n",
    "\n",
    "# We can't easily load a LoRA without a file, but we can look at the config class\n",
    "from peft import LoraConfig\n",
    "\n",
    "config = LoraConfig(r=4, lora_alpha=4, target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"])\n",
    "\n",
    "print(f\"LoRA Rank (r): {config.r}\")\n",
    "print(f\"Target Modules: {config.target_modules}\")\n",
    "print(\"When we train, we only train matrices of size (dim x 4) and (4 x dim)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step-by-Step Visualization\n",
    "\n",
    "How does the image emerge? Let's check the latent at step 10 vs step 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function to capture intermediates\n",
    "latents_history = []\n",
    "\n",
    "def decode_latents(latents, pipe):\n",
    "    \"\"\"Helper to decode latents to images\"\"\"\n",
    "    with torch.no_grad():\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = pipe.vae.decode(latents).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        # Convert to PIL\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        return [Image.fromarray(img) for img in image]\n",
    "\n",
    "def callback(pipe, step, timestep, callback_kwargs):\n",
    "    # Capture every 10 steps\n",
    "    if step % 10 == 0:\n",
    "        latents = callback_kwargs.get(\"latents\")\n",
    "        if latents is not None:\n",
    "             # Decode and store (only the first image in batch)\n",
    "             images = decode_latents(latents, pipe)\n",
    "             latents_history.append((step, images[0]))\n",
    "    return callback_kwargs\n",
    "\n",
    "print(\"Generating with step visualization...\")\n",
    "prompt = \"a cute robot painting a canvas, high resolution\"\n",
    "\n",
    "# Get pipeline and run\n",
    "pipe = pipeline_manager.get_txt2img_pipeline()\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# We need to use a generator for reproducibility\n",
    "generator = torch.Generator(device=pipe.device).manual_seed(42)\n",
    "\n",
    "# Run generation with callback\n",
    "output = pipe(\n",
    "    prompt=prompt, \n",
    "    num_inference_steps=31, \n",
    "    generator=generator,\n",
    "    callback_on_step_end=callback\n",
    ").images[0]\n",
    "\n",
    "print(f\"Captured {len(latents_history)} intermediate steps.\")\n",
    "\n",
    "# Visualize\n",
    "fig, axs = plt.subplots(1, len(latents_history) + 1, figsize=(20, 5))\n",
    "\n",
    "# Plot intermediates\n",
    "for i, (step, img) in enumerate(latents_history):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(f\"Step {step}\")\n",
    "    axs[i].axis('off')\n",
    "\n",
    "# Plot final\n",
    "axs[-1].imshow(output)\n",
    "axs[-1].set_title(\"Final Result\")\n",
    "axs[-1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Math of Guidance (Simulator)\n",
    "\n",
    "Classifier-Free Guidance (CFG) is the \"Magic Spell\" of Stable Diffusion.\n",
    "\n",
    "$$ \\epsilon_{final} = \\epsilon_{uncond} + w \\cdot (\\epsilon_{cond} - \\epsilon_{uncond}) $$\n",
    "\n",
    "Let's visualize this with a simple 2D vector simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CFG SIMULATOR ===\n",
    "# Imagine our image is just a point in 2D space for simplicity.\n",
    "\n",
    "def simulate_cfg(weight):\n",
    "    # 1. Unconditioned Prediction (Model guessing without prompt)\n",
    "    # It pushes towards generic \"average\" images\n",
    "    uncond_vector = np.array([2.0, 1.0]) \n",
    "\n",
    "    # 2. Conditioned Prediction (Model pushing towards \"Red Apple\")\n",
    "    cond_vector = np.array([4.0, 5.0])\n",
    "\n",
    "    # 3. The Difference (The \"Concept\" of Red Apple)\n",
    "    # This vector represents pure \"Red Apple-ness\" without the generic image parts\n",
    "    concept_vector = cond_vector - uncond_vector\n",
    "\n",
    "    # 4. Final Vector\n",
    "    final_vector = uncond_vector + weight * concept_vector\n",
    "    \n",
    "    return uncond_vector, cond_vector, final_vector\n",
    "\n",
    "# Visualize various weights\n",
    "weights = [1.0, 7.0, 15.0]\n",
    "colors = ['green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.axvline(0, color='gray', alpha=0.3)\n",
    "plt.axhline(0, color='gray', alpha=0.3)\n",
    "plt.xlim(0, 40)\n",
    "plt.ylim(0, 65)\n",
    "\n",
    "for i, w in enumerate(weights):\n",
    "    uncond, cond, final = simulate_cfg(w)\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.arrow(0, 0, final[0], final[1], head_width=1, head_length=1, fc=colors[i], ec=colors[i], label=f'CFG {w} (Result)')\n",
    "    \n",
    "    # Plot the Original Conditioned (Reference)\n",
    "    if i == 0:\n",
    "        plt.arrow(0, 0, cond[0], cond[1], head_width=0.5, head_length=0.5, fc='blue', ec='blue', alpha=0.5, linestyle=':', label='Raw Prompt Target')\n",
    "        plt.arrow(0, 0, uncond[0], uncond[1], head_width=0.5, head_length=0.5, fc='gray', ec='gray', alpha=0.5, linestyle=':', label='Unconditional Base')\n",
    "\n",
    "plt.title(\"Visualizing Classifier-Free Guidance Vectors\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"- CFG 1.0 (Green): Exactly matches the raw prompt target.\")\n",
    "print(\"- CFG 7.0 (Orange): Extrapolates FURTHER in that direction. This is 'Standard' SD.\")\n",
    "print(\"- CFG 15.0 (Red): Pushes extremely far. This can lead to 'frying' (oversaturation).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nbformat": 4,
  "nbformat_minor": 2
 }
}
