{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Lesson 1: The Physics of Stable Diffusion (From Scratch)\n",
    "\n",
    "Welcome to the **Deep Dive**! Most courses teach you how to \"drive\" the car (using the `StableDiffusionPipeline`). Today, we are going to **build the engine**.\n",
    "\n",
    "We will manually orchestrate the neural networks to generate an image, explaining the **science and math** at every step.\n",
    "\n",
    "### ðŸ”¬ What we will explore:\n",
    "1.  **The Manifold Hypothesis**: Why we need a VAE to compress images.\n",
    "2.  **CLIP Embeddings**: Converting language into high-dimensional vectors.\n",
    "3.  **The Physics of Diffusion**: How reversing a thermodynamic process generates art.\n",
    "4.  **The Denoising Loop**: Manually stepping through the differential equation solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Import necessary libraries\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Boilerplate to import our local config\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "from config import device_config\n",
    "\n",
    "device = device_config.get_device()\n",
    "dtype = device_config.dtype\n",
    "print(f\"Using device: {device}, dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Components of Creation\n",
    "\n",
    "Stable Diffusion isn't one model; it's a team of three specialized neural networks working together.\n",
    "\n",
    "### ðŸ§  The Team:\n",
    "1.  **VAE (Variational Autoencoder)**: The *Data Compressor*. It shrinks huge images (512x512) into tiny \"latent interactions\" (64x64) so they fit in GPU memory.\n",
    "2.  **Text Encoder (CLIP)**: The *Translator*. It turns English text into a 768-dimensional numerical coordinate system.\n",
    "3.  **U-Net**: The *Artist*. A massive ResNet that predicts noise. It takes a noisy latent + text embeddings and outputs \"estimated noise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# 1. Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "\n",
    "# 2. Load Text Encoder & Tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=dtype).to(device)\n",
    "\n",
    "# 3. Load U-Net\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", torch_dtype=dtype).to(device)\n",
    "\n",
    "# 4. Load Scheduler (The Math Solver)\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "\n",
    "print(\"âœ… All systems loaded directly into VRAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text to Vector Space (Math of Language)\n",
    "\n",
    "Computers don't understand words; they understand vectors. We use **CLIP** (Contrastive Language-Image Pre-Training).\n",
    "\n",
    "CLIP maps images and text to the *same* high-dimensional space. If the vector for \"dog\" is `[0.1, 0.9, ...]`, the U-Net uses these numbers to guide its generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a cyberpunk detective standing in rain, neon lights, highly detailed, 8k\"]\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5 # Controls how strictly we follow the prompt\n",
    "generator = torch.manual_seed(42)\n",
    "batch_size = len(prompt)\n",
    "\n",
    "# --- DEEP DIVE: What does the model actually see? ---\n",
    "\n",
    "# 1. Tokenization: Break text into tokens (numbers)\n",
    "# The tokenizer has a vocabulary of ~49k sub-words.\n",
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"MATCHING WORDS TO NUMBERS:\")\n",
    "print(f\"Raw Input: '{prompt[0]}'\")\n",
    "\n",
    "# Let's peek at the first 15 tokens\n",
    "raw_ids = text_input.input_ids[0][:15]\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(raw_ids)\n",
    "\n",
    "for token_id, token_str in zip(raw_ids, decoded_tokens):\n",
    "    print(f\"  {token_id.item():<5} -> '{token_str}'\")\n",
    "    \n",
    "print(f\"\\nTotal Sequence Length: {text_input.input_ids.shape[1]} (Standard for SD is 77 tokens)\")\n",
    "print(\"Notice the starting '<|startoftext|>' and ending '<|endoftext|>'? These are crucial flags for the model.\")\n",
    "print(\"Everything after the end token is 'padding' (49407) which the model ignores.\\n\")\n",
    "\n",
    "# 2. Encoding: Convert tokens to 768-dim Vectors\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
    "\n",
    "# 3. Classifier-Free Guidance (The Magic Trick)\n",
    "# We also generate an \"empty\" embedding. We will later calculate:\n",
    "# Final = Empty + Scale * (Text - Empty)\n",
    "# This pushes the image AWAY from \"generic/boring\" and TOWARDS our prompt.\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "# Stitch them together for batch processing\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "print(f\"Embedding Shape: {text_embeddings.shape} (2 batches x 77 tokens x 768 dimensions)\")\n",
    "print(\"We have successfully translated English into 'Math'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Latent Space & Initial Noise\n",
    "\n",
    "### Why Latent Space?\n",
    "Generating a 512x512 image means manipulating **786,432 pixels** ($512 \\times 512 \\times 3$). This is too slow.\n",
    "Instead, we work in **Latent Space** ($64 \\times 64 \\times 4 = 16,384$ values). \n",
    "This is a **48x compression**! The VAE handles the translation.\n",
    "\n",
    "We start with pure Gaussian noise $N(0, I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=device,\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "# Scale noise for the scheduler (Sigma)\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "print(f\"Latent Shape: {latents.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Denoising Loop (Solving the ODE)\n",
    "\n",
    "Diffusion is a **physics simulation**. We are reversing entropy.\n",
    "\n",
    "The equation we are solving is the **Probability Flow ODE**.\n",
    "$$ dx = -\\dot{\\sigma}(t) \\sigma(t) \\nabla_x \\log p_t(x) dt $$\n",
    "\n",
    "In English: \"Move the data point $x$ against the gradient of noise density.\"\n",
    "\n",
    "We will visualize the image decoding every 5 steps so you can see the structure emerging from chaos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "def decode_latents(latents):\n",
    "    # Helper to peek at the image during generation\n",
    "    l = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(l).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "    return image[0]\n",
    "\n",
    "print(\"Starting Denoising Loop...\")\n",
    "\n",
    "for i, t in enumerate(pattern for pattern in tqdm(scheduler.timesteps)):\n",
    "    # 1. Expand latents for our double-pass (Conditioned + Unconditioned)\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "    # 2. U-Net Prediction: \"What part of this image is noise?\"\n",
    "    # This involves massive matrix multiplications in the ResNet blocks and Attention layers\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # 3. Classifier-Free Guidance Math\n",
    "    # epsilon_pred = epsilon_uncond + s * (epsilon_text - epsilon_uncond)\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # 4. Scheduler Step: Update x_t -> x_{t-1}\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    \n",
    "    # Visualization every 10 steps\n",
    "    if (i + 1) % 5 == 0:\n",
    "        img = decode_latents(latents)\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Step {i+1}/{num_inference_steps}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Decode\n",
    "\n",
    "The loop finishes with a clean latent. We pass it through the VAE Decoder to get our final pixel-perfect image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image_arr = decode_latents(latents)\n",
    "final_image = Image.fromarray((final_image_arr * 255).round().astype(\"uint8\"))\n",
    "final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Independent Study\n",
    "\n",
    "Try changing the variables above to explore the science:\n",
    "\n",
    "1.  **Guidance Scale**: Change `guidance_scale` to `1.0`. The image will look good but might not match the prompt. Why?\n",
    "2.  **Timesteps**: Change `num_inference_steps` to `5`. It will look blurry/noisy. This is **under-sampling** the differential equation.\n",
    "3.  **Seed**: Change the `seed` to get a different noise pattern, resulting in a completely different composition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nbformat": 4,
  "nbformat_minor": 2
 }
}
