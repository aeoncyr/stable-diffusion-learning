{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udfe3 Lesson 15: LoRA Theory\n",
    "\n",
    "Welcome to **Module 5: Training & Customization**. We are going to teach the model new things.\n",
    "\n",
    "### The Problem: Fine-Tuning is Expensive\n",
    "Stable Diffusion has **860 Million** parameters. Training all of them requires ~24GB VRAM and days of time.\n",
    "\n",
    "### The Solution: LoRA (Low-Rank Adaptation)\n",
    "Instead of updating the massive weight matrix $W$ ($d \\times d$), we inject two tiny matrices $A$ ($r \\times d$) and $B$ ($d \\times r$).\n",
    "$$ W' = W + B \\times A $$\n",
    "\n",
    "If $d=768$ and $r=4$:\n",
    "- Full Fine-Tune: $768 \\times 768 = 590,000$ params.\n",
    "- LoRA: $(768 \\times 4) + (4 \\times 768) = 6,000$ params.\n",
    "\n",
    "That is **100x fewer parameters** to train, but it achieves nearly the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "import notebook_utils\n",
    "project_root, device, dtype = notebook_utils.setup_notebook()\n",
    "\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining a LoRA Config\n",
    "\n",
    "We use the **Hugging Face PEFT** library (Parameter-Efficient Fine-Tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=4,       # Rank: Lower = smaller file, Higher (8, 16) = more detail\n",
    "    lora_alpha=4, # Scaling factor\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"], # We attach LoRA to the Attention layers\n",
    ")\n",
    "\n",
    "print(f\"LoRA Rank: {config.r}\")\n",
    "print(f\"Target Modules: {config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization\n",
    "\n",
    "Think of LoRA as a \"Filter\" you slide over the model. \n",
    "- Base Model: \"Draw a dog.\"\n",
    "- Base + Anime LoRA: \"Draw an anime dog.\"\n",
    "- Base + Pixel Art LoRA: \"Draw a pixel art dog.\"\n",
    "\n",
    "You can even mix them! (0.5 Anime + 0.5 Pixel Art)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nbformat": 4,
  "nbformat_minor": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}