{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udd35 Lesson 4: VAE and Latent Space\n",
    "\n",
    "We are entering **Module 2: Architecture Deep Dive**. We start with the Variational Autoencoder (VAE).\n",
    "\n",
    "### The Problem: Images are Big\n",
    "A 512x512 image has `512 * 512 * 3 (RGB) = 786,432` pixels.\n",
    "Trying to run attention mechanisms on nearly a million pixels would crash even a 4090.\n",
    "\n",
    "### The Solution: Latent Space\n",
    "Images have a lot of redundancy (the sky is mostly blue, the wall is flat). We can compress them.\n",
    "Stable Diffusion compresses images by **factor of 8**: `512 / 8 = 64`.\n",
    "The new size is `64 * 64 * 4 (Channels) = 16,384` values.\n",
    "\n",
    "This is **48x smaller** than the original image! This allows the AI to be fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "import notebook_utils\n",
    "project_root, device, dtype = notebook_utils.setup_notebook()\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n",
    "print(\"VAE Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compress an Image (Encode)\n",
    "Let's take our mountain image from Lesson 1 and squeeze it into latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "response = requests.get(url)\n",
    "image_pil = Image.open(BytesIO(response.content)).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# Preprocess for VAE (Convert to -1 to 1 range)\n",
    "image_arr = np.array(image_pil).astype(np.float32) / 127.5 - 1.0\n",
    "image_tensor = torch.from_numpy(image_arr).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"Original Shape: {image_tensor.shape} (Batch, Channel, Height, Width)\")\n",
    "\n",
    "# ENCODE\n",
    "with torch.no_grad():\n",
    "    latents = vae.encode(image_tensor).latent_dist.sample()\n",
    "    \n",
    "# Scale factor (Magic number for SD 1.5)\n",
    "latents = latents * 0.18215\n",
    "\n",
    "print(f\"Latent Shape:   {latents.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Latent Space\n",
    "\n",
    "The latent tensor has 4 channels. We can't view it as a normal image, but we can visualize the channels separately to see what \"concepts\" the VAE found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "l_vis = latents[0].cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(l_vis[i], cmap='viridis')\n",
    "    axs[i].set_title(f\"Latent Channel {i+1}\")\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decompress (Decode)\n",
    "\n",
    "Now we reverse the process. Even though we threw away 98% of the data, the VAE can hallucinate the missing details back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscale\n",
    "latents = latents / 0.18215\n",
    "\n",
    "# DECODE\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(latents).sample\n",
    "\n",
    "# Post-process (Back to 0-255)\n",
    "decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "final_image = Image.fromarray((decoded_image[0] * 255).astype(np.uint8))\n",
    "\n",
    "notebook_utils.show_image(final_image, title=\"Reconstructred from Latent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nbformat": 4,
  "nbformat_minor": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}